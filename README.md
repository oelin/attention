# Attention

Different kinds of attention implemented in PyTorch and other frameworks (WIP).

## 1. Introduction 

Attention is a powerful operator that enables neural networks to context-dependently mix different parts of an input in order to obtain more meaningful representations. This project aims to implement some of the many attention varieties that now exist.
